{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDPP Course lab 1\n",
    "\n",
    "Welcome to the first lab of BDPP course!\n",
    "\n",
    "This notebook guides you through the basic concepts to start working with Spark, including how to set up your environment, create and analyze data sets, work with data files, and write Spark code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Spark\n",
    "Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for processing structured data, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "<img src='files/spark.png' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "\n",
    "A Spark program has a driver program and worker programs. Worker programs run on cluster nodes or in local threads. Data sets are distributed\u001d",
    " across workers. \n",
    "\n",
    "<img src='files/Spark Architecture.png' width=\"50%\" height=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sparkinstallation\"></a>\n",
    "## 1. Three methods of running Spark\n",
    "\n",
    "The first step is to get familiar with different ways of accessing the Spark programming environment. In this lab, we introduce you three methods to do so (two of them run on your local computer, and the other one uses the IBM cloud platform). You are required to run this notebook on your local computer. Alternatively, The cloud platform can be utilized for the cases where you want to scale up your process.\n",
    "\n",
    "<a id=\"sparkinstallation1\"></a>\n",
    "\n",
    "\n",
    "### 1.1 Set up a local environment\n",
    "\n",
    "This notebook uses pySpark, the Python API for Spark. You should follow the following steps to install PySpark locally:\n",
    "- Install Python\n",
    "- Install Java\n",
    "- Download Spark\n",
    "- Install pyspark\n",
    "\n",
    "#### Install Python\n",
    "\n",
    "We use **Python** as the programming language, **Anaconda** as the virtual environment and package manager, and finally **Jupyter notebook** as the development environment. If you haven't installed Python yet, we recommend you to install it through [Anaconda](https://anaconda.org/). Note that this lab is not intended to teach python programming. So, it is recommended to have some python programming background.\n",
    "\n",
    "Please follow these steps to prepare the development environment:\n",
    "- Download Anaconda (Python 3.7 version) from https://www.anaconda.com/distribution/, and install it on your machine.\n",
    "- Create an environment with Python 3.7 and use `pp_course` as the environment name. You can find information about creating and managing anaconda environments using navigator interface from https://docs.anaconda.com/anaconda/navigator/tutorials/manage-environments/#creating-a-new-environment. You can also do it through shell commands https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html. Here is a list of useful commands:\n",
    "    - `conda create -n pp_course python=3.7`\n",
    "    - `conda env list`\n",
    "    - `source activate pp_course` for unix/mac, and `conda activate pp_course` for windows\n",
    "- Jupyter editor should be installed on your environment by default. If it is not, use Anaconda navigator to install it. To do so, first, go to the Home tab, then select pp_course environment. Finally, from the list of applications, find Jupyter Notebook, and install it. \n",
    "- Now you should be able to launch Jupyter. This launches a new browser window (or a new tab) showing the notebook Dashboard, a sort of control panel that allows (among other things) to select which notebook to open. You can also launch Jupyter using shell command. To do so, first, activate `pp_course` environment, then run the following command: `jupyter notebook`\n",
    "- Play with the interface and try to figure out its functionalities. You can find lots of useful information about notebook from the following [link](https://buildmedia.readthedocs.org/media/pdf/jupyter-notebook/latest/jupyter-notebook.pdf).\n",
    "\n",
    "#### Install Java\n",
    "\n",
    "You need Java installed on your machine since Spark runs on top of JVM. Download and install Java SDK 8. You can download it from the following link:\n",
    "http://www.oracle.com/technetwork/pt/java/javase/downloads/jdk8-downloads-2133151.html\n",
    "\n",
    "Add `JAVA_HOME` environment variable to your system (**Note:** This step depends on your operating system.):\n",
    "- on Unix/mac: `export JAVA_HOME=\"/path/to/the/java/folder\"`\n",
    "- on Windows see this [link](https://confluence.atlassian.com/doc/setting-the-java_home-variable-in-windows-8895.html)\n",
    "\n",
    "#### Download Spark\n",
    "\n",
    "Spark is an open-source project under Apache Software Foundation. Spark can be downloaded here:\n",
    "\n",
    "https://spark.apache.org/downloads.html\n",
    "\n",
    "First, choose a Spark release, e,g, 2.4.X, and choose pre-build for Apache Hadoop, e,g, 2.7. Next, click on the download link and download the file. We recommend moving the file to your home directory, uncompress it, and maybe rename it to a shorter name such as `spark`. Now the spark file should be located here.\n",
    "\n",
    "`/your/home/directory/spark`\n",
    "\n",
    "Add `SPARK_HOME` environment variable to your system (**Note:** This step depends on your operating system):\n",
    "\n",
    "- on Unix/mac, e.g.: `export SPARK_HOME=\"/your/home/directory/spark/python\"` and then `export PATH=\"$SPARK_HOME/bin:$PATH\"`\n",
    "- on Windows: Similarly to the Java installation, set `SPARK_HOME` and add `%SPARK_HOME%\\bin` in PATH variable in System Variables.\n",
    "\n",
    "<a id=\"sparkinstallation2\"></a>\n",
    "\n",
    "Now, launch your terminal and run the command below to test the installation.\n",
    "\n",
    "`spark-shell`\n",
    "\n",
    "You should be able to see this:\n",
    "\n",
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n",
    "      /_/\n",
    "```\n",
    "#### Install pyspark\n",
    "\n",
    "The final step is to install pyspark. Run the following command to install pyspark. (if you are using python without Anaconda, use the following alternative command: `pip install pyspark`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing pyspark package in the current Jupyter kernel\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} -c conda-forge pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Use a docker image\n",
    "\n",
    "Perhaps one of the easiest approaches for setting up a Spark environment is to use an available docker image. Setting up a Docker container on your local machine is pretty simple.  Download docker from the [docker website](https://www.docker.com/get-started) and run the following command in the terminal:\n",
    "\n",
    "`docker run -it -p 8888:8888 jupyter/pyspark-notebook`\n",
    "\n",
    "navigate to http://localhost:8888 in your browser, and you see the following screen:\n",
    "\n",
    "<img src=files/docker1.png width=\"500\">\n",
    "\n",
    "In your terminal, you should see a token:\n",
    "\n",
    "<img src=files/docker2.png width=\"500\">\n",
    "\n",
    "Copy and paste this token, the numbers following `/?token=`, into the token textbook. With that done, you are all set to go! Spark is already installed in the container. You are all ready to open up a notebook and start writing some Spark code. \n",
    "\n",
    "<a id=\"sparkinstallation3\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Use a cloud platform\n",
    "\n",
    "The final approach that we introduce for accessing Spark is through a cloud platform. The cloud platform, along with Spark enable you to scale up your application according to your demands easily. In this section, we use the IBM cloud as our platform and IBM Watson Studio as our developing environment. The first step is to go to https://www.ibm.com/cloud and create a free account.\n",
    "<img src='files/ibm cloud1.png' width=\"800\">\n",
    "\n",
    "Next, log in to your account and confirm the required Acknowledgements by IBM. Now, you should be able to see your dashboard. According to the image below, search for Watson studio and run it.\n",
    "\n",
    "<img src='files/search watson.png' width=\"800\">\n",
    "\n",
    "Next, select the Lite plan and create your kernel. Remember that you can have only one instance of a Lite plan per service. To create a new instance, you have to delete your existing Lite plan instance.\n",
    "\n",
    "<img src='files/select watson plan.png' width=\"800\">\n",
    "\n",
    "And get started working.\n",
    "\n",
    "<img src='files/watson start2.png' width=\"800\">\n",
    "\n",
    "Now you must be able to create a Watson Studio project and add an empty notebook to your project. This opens an empty jupyter notebook with SparkContext available for use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sparkcontext\"></a>\n",
    "## 2. Work with the SparkContext and SparkSession objects\n",
    "\n",
    "The Spark driver application uses the SparkContext object to allow a programming interface to interact with the driver application. The SparkContext object tells Spark how and where to access a cluster.\n",
    "\n",
    "The Watson Studio notebook environment predefines the Spark context for you. The object name to access the Spark session is `sc`.\n",
    "\n",
    "In other environments, you need to pick an interpreter (for example, pyspark for Python) and create a SparkConf object to initialize a SparkContext object. For example:\n",
    "<br>\n",
    "`from pyspark import SparkContext, SparkConf`<br>\n",
    "`conf = SparkConf().setAppName(appName).setMaster(master)`<br>\n",
    "`sc = SparkContext(conf=conf)`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sparkcontext1\"></a>\n",
    "### 2.1 Invoke the SparkContext\n",
    "Run the following cell to invoke the SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost.localdomain:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Lab1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark Lab1>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('Spark Lab1')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remember__ that you can create only one instance of the `SparkContext` object (`sc`) in each pyspark session. This means that if you run the above code snippet again, you get the following error:\n",
    "\n",
    "`ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=My app, master=local) created by __init__ ... ` \n",
    "\n",
    "Moreover, you have to close your Spark session at the end of your program. This can be done by calling the `stop` function: `sc.stop()`. It ensures that you will not face any problem connecting to Spark again.\n",
    "\n",
    "The same goes for the `SparkSession` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sparkcontext2\"></a>\n",
    "### 2.2 Check the Spark version\n",
    "Check the version of the Spark driver application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.0-preview2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 SparkSession\n",
    "\n",
    "Prior to Spark 2.0.0, `sparkContext` was used as a channel to access all spark functionality. From Spark 2.0.0 onwards, `SparkSession` provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with DataFrame and Dataset APIs.\n",
    "\n",
    "The Watson Studio notebook environment predefines the Spark session for you. The object name to access Spark session is `spark`.\n",
    "\n",
    "In other environments, you need to pick an interpreter (for example, pyspark for Python) and create a Spark session object. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark Lab1\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost.localdomain:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Lab1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9185588520>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the functionality available with sparkContext are also available in sparkSession. If you need to access `SparkContext` through SparkSession use `sparkContext` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost.localdomain:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Lab1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Spark Lab1>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Speed benchmark (Spark VS Pandas)\n",
    "\n",
    "Now, let's use the `sparkSession` object to run a simple benchmark by comparing reading a relatively big CSV file with pandas VS Spark. Although we are still running Spark on our local computer, it ends up reading in the CSV much faster than pandas. This demonstrates how Spark dataframes are much faster when compared to their pandas equivalent.\n",
    "\n",
    "For this experiment, we use a somewhat large Vermont vendor dataset. This data is accessible through [this link](https://data.vermont.gov/Finance/Vermont-Vendor-Payments/786x-sbp3). On this link, please select export and then choose CSV format. Download the file rename it to `Vermont_Vendor_Payments.csv` and place it in the `files` folder next to this notebook. Now, run the following two code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127 ms ± 16.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# loading csv file with Spark\n",
    "housing = spark.read.csv(\"./files/Vermont_Vendor_Payments.csv\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installl pandas library if you don't have it.\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.48 s ± 9.91 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "# loading csv file with Pandas\n",
    "df_pandas = pd.read_csv(\"files/Vermont_Vendor_Payments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: Based on your experiment, how much faster does Spark run compared to Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" TODO: change the format of this cell to Markdown and answer the quesion here \"\"\"\n",
    "\n",
    "\n",
    "Roughly 19 times faster. $\\frac{2.48}{0.127}\\approx19$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Speed benchmark (π calculation)\n",
    "\n",
    "Spark can also be used for compute-intensive tasks. This code estimates π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4, so we use this to get our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# π calculation code\n",
    "\n",
    "import random\n",
    "\n",
    "num_samples = 10000000 # you can change this number, e.g. try 1000000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "def spark_pi_calc():\n",
    "    # here we do the pi calcaulation using Spark\n",
    "    count = spark.sparkContext.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "    return (4.0 * count / num_samples)\n",
    "\n",
    "def python_pi_calc():\n",
    "    # here we do the same calculation with python list comprehension\n",
    "    count = sum([inside(_) for throw in range(num_samples)])\n",
    "    return (4.0 * count / num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] Pi is roughly: 3.1413188\n"
     ]
    }
   ],
   "source": [
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "print(\"[Spark] Pi is roughly:\", spark_pi_calc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Python] Pi is roughly: 3.1418108\n"
     ]
    }
   ],
   "source": [
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "print(\"[Python] Pi is roughly:\", python_pi_calc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use π calculation code to benchmark spark VS python. For small problems, python might work faster than spark because of the initial setup cost of spark. However, as the problem gets bigger, the spark code starts to show its benefit and runs faster than python.\n",
    "\n",
    "In the code below, we start from a small `num_samples` and keep doubling it until the python loop exceeds `max_time` (here it is set to 3 seconds by default). We collect running times for spark and python codes and produce a plot displaying time VS num_samples. \n",
    "\n",
    "We want you to play with the `max_time` parameter until the problem gets big enough so that you observe the spark code runs faster than the python code. This, of course, highly depends on the parallelization capacity of your CPU, and you may end up getting different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment. This may take a few minutes to run.\n",
      "You can change max_time value to increase or decrease run time.\n",
      "(please wait)\n",
      "0 / 1000\n",
      "2 / 1000\n",
      "24 / 1000\n",
      "47 / 1000\n",
      "94 / 1000\n",
      "185 / 1000\n",
      "385 / 1000\n",
      "770 / 1000\n",
      "1000 / 1000\n",
      "Done! Total time = 122.76s\n"
     ]
    }
   ],
   "source": [
    "import timeit, time\n",
    "\n",
    "max_time = 10 # you can also try 1, 2, 5, and 10 depending on your hardware performance.\n",
    "\n",
    "print('Running experiment. This may take a few minutes to run.')\n",
    "print('You can change max_time value to increase or decrease run time.')\n",
    "print('(please wait)')\n",
    "\n",
    "num_samples = 10000\n",
    "steps = []\n",
    "python_times = []\n",
    "sparks_times = []\n",
    "\n",
    "def my_timeit(func):\n",
    "    runs = 3  # If the experiment is still taking too much time to run, you may decrease this value as well.\n",
    "    dtime = timeit.timeit(func, number=runs)\n",
    "    elapsed = dtime/runs\n",
    "    return elapsed\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    pt = my_timeit(python_pi_calc)\n",
    "    st = my_timeit(spark_pi_calc)\n",
    "    python_times.append(pt)\n",
    "    sparks_times.append(st)\n",
    "    steps.append(num_samples)\n",
    "    print(min(int(pt * 100), max_time*100), '/', max_time*100)\n",
    "    if pt > max_time:\n",
    "        break\n",
    "        \n",
    "    if pt > max_time:\n",
    "        break\n",
    "    elif pt < 0.1:\n",
    "        num_samples = num_samples * 10\n",
    "    else:\n",
    "        num_samples = num_samples * 2\n",
    "print(f\"Done! Total time = {time.time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps, python_times, color='red', label='python')\n",
    "plt.plot(steps, sparks_times, color='blue', label='spark')\n",
    "plt.legend()\n",
    "plt.xlabel('number of samples', fontsize=12)\n",
    "plt.ylabel('running time (seconds)', fontsize=12)\n",
    "plt.title('Speed benchmark (π calculation)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: How does Spark run faster than (Pandas/Python) even on a single computer? What does the slope of the blue line in the generated plot tell you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" TODO: change the format of this cell to Markdown and answer the quesion here \"\"\"\n",
    "\n",
    "\n",
    "It's because spark can utilize all cores by paralellizing the execution, while python runs on a single core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hint__: If you pick a big enough value for the `num_samples` parameter (code below), you should be able to see multiple python processes running at the same time in your (system monitor/task manager) when benchmarking spark code (A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1415882"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code (A) Spark \n",
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "num_samples = 1000000000\n",
    "spark_pi_calc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.14157968"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code (B) Python - List Comprehension\n",
    "# Depending on your hardware, this can take some time to fininsh.\n",
    "# You can reduce num_samples if it is taking too much time.\n",
    "\n",
    "num_samples = 100000000\n",
    "python_pi_calc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<img src=\"files/spark run.png\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "This picture also demonstrates how Spark uses CPU VS python:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"files/spark cpu load.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd\"></a>\n",
    "## 3. Work with Resilient Distributed Datasets\n",
    "Spark uses an abstraction for working with data called a Resilient Distributed Dataset (RDD). An RDD is a collection of elements that can be operated on in parallel. RDDs are immutable, so you can't update the data in them. To update data in an RDD, you must create a new RDD. In Spark, all work is done by creating new RDDs, transforming existing RDDs, or using RDDs to compute results. When working with RDDs, the Spark driver application automatically distributes the work across the cluster.\n",
    "\n",
    "You can construct RDDs by parallelizing existing Python collections (lists), by manipulating RDDs, or by manipulating files in HDFS or any other storage system.\n",
    "\n",
    "You can run these types of methods on RDDs: \n",
    " - Actions: query the data and return values\n",
    " - Transformations: manipulate data values and return pointers to new RDDs. \n",
    "\n",
    "Find more information on Python methods in the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html\" target=\"_blank\" rel=\"noopener noreferrer\">PySpark documentation</a>.\n",
    "\n",
    "<a id=\"rdd1\"></a>\n",
    "### 3.1 Create a collection\n",
    "Create a Python collection of the numbers 1 - 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd2\"></a>\n",
    "### 3.2 Create an RDD \n",
    "Put the collection into an RDD named `x_nbr_rdd` using the `parallelize` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nbr_rdd = spark.sparkContext.parallelize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there's no return value. The `parallelize` method didn't compute a result, which means it's a transformation. Spark just recorded how to create the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd3\"></a>\n",
    "### 3.3 View the data \n",
    "View the first element in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number in the collection is in a different element in the RDD. Because the `first()` method returned a value, it is an action. \n",
    "\n",
    "Now view the first five elements in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rdd4\"></a>\n",
    "### 3.4 Create another RDD \n",
    "Create a Python collection that contains strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [\"Hello Human\", \"My Name is Spark\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the collection into an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_str_rdd = spark.sparkContext.parallelize(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the first element in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Human']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_str_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You created the string \"Hello Human\" and you returned it as the first element of the RDD. To analyze a set of words, you can map each word into an RDD element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans\"></a>\n",
    "## 4. Manipulate data in RDDs\n",
    "\n",
    "Remember that to manipulate data, you use transformation functions.\n",
    "\n",
    "Here are some common Python transformation functions that you'll be using in this notebook:\n",
    "\n",
    " - `map(func)`: returns a new RDD with the results of running the specified function on each element  \n",
    " - `filter(func)`: returns a new RDD with the elements for which the specified function returns true   \n",
    " - `distinct([numTasks]))`: returns a new RDD that contains the distinct elements of the source RDD\n",
    " - `flatMap(func)`: returns a new RDD by first running the specified function on all elements, returning 0 or more results for each original element, and then flattening the results into individual elements\n",
    "\n",
    "You can also create functions that run a single expression and don't have a name with the Python `lambda` keyword. For example, this function returns the sum of its arguments: `lambda a , b : a + b`.\n",
    "\n",
    "<a id=\"trans1\"></a>\n",
    "### 4.1 Update numeric values\n",
    "Run the `map()` function with the `lambda` keyword to replace each element, X, in your first RDD (the one that has numeric values) with X+1. Because RDDs are immutable, you need to specify a new RDD name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "x_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the elements of the new RDD: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_nbr_rdd_2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with the `collect` method! It returns __all__ elements of the RDD to the driver. Returning a large data set might be not be very useful. No-one wants to scroll through a million rows!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans2\"></a>\n",
    "### 4.2 Add numbers in an array\n",
    "An array of values is a common data format where multiple values are contained in one element. You can manipulate the individual values if you split them up into separate elements.\n",
    "\n",
    "Create an array of numbers by including quotation marks around the whole set of numbers. If you omit the quotation marks, you get a collection of numbers instead of an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"1,2,3,4,5,6,7,8,9,10\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD for the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rd = spark.sparkContext.parallelize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the values at commas and add values in the positions 3 and 7 in the array. Keep in mind that an array starts with position 0. Use a backslash character, \\, to break the line of code for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "Sum_rd = y_rd.map(lambda y: y.split(\",\")).map(lambda y: int(y[3]) + int(y[7]))\n",
    "Sum_rd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now return the value of the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sum_rd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `12`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans3\"></a>\n",
    "### 4.3 Split and count text strings\n",
    "\n",
    "Create an RDD with a text string and show the first element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Human. I'm Spark and I love running analysis on data.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words = [\"Hello Human. I'm Spark and I love running analysis on data.\"]\n",
    "words_rd = spark.sparkContext.parallelize(Words)\n",
    "words_rd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the string into separate lines at the space characters and look at the first element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Human.',\n",
       " \"I'm\",\n",
       " 'Spark',\n",
       " 'and',\n",
       " 'I',\n",
       " 'love',\n",
       " 'running',\n",
       " 'analysis',\n",
       " 'on',\n",
       " 'data.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "Words_rd2 = words_rd.map(lambda line: line.split(\" \"))\n",
    "Words_rd2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of elements in this RDD with the `count()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words_rd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you already knew that there was only one element because you ran the `first()` method and it returned the whole string. Splitting the string into multiple lines did not create multiple elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split the string again, but this time with the `flatmap()` method, and look at the first three elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Human.', \"I'm\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "words_rd2 = words_rd.flatMap(lambda line: line.split(\" \"))\n",
    "words_rd2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `11`.\n",
    "This time each word is separated into its own element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"trans4\"></a>\n",
    "### 4.4 Count words with a pair RDD\n",
    "A common way to count the number of instances of words in an RDD is to create a pair RDD. A pair RDD converts each word into a key-value pair: the word is the key and the number 1 is the value. Because the values are all 1, when you add the  values for a particular word, you get the number of instances of that word.\n",
    "\n",
    "Create an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First,Line'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]\n",
    "z_str_rdd = spark.sparkContext.parallelize(z)\n",
    "z_str_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the elements into individual words with the `flatmap()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First', 'Line', 'Second', 'Line', 'and', 'Third', 'Line']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\"))\n",
    "z_str_rdd_split_flatmap.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the elements into key-value pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('First', 1),\n",
       " ('Line', 1),\n",
       " ('Second', 1),\n",
       " ('Line', 1),\n",
       " ('and', 1),\n",
       " ('Third', 1),\n",
       " ('Line', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))\n",
    "countWords.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now sum all the values by key to find the number of instances for each word: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Line', 3), ('Second', 1), ('First', 1), ('and', 1), ('Third', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "countWords2 = countWords.reduceByKey(add)\n",
    "countWords2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the word `Line` has a count of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filter\"></a>\n",
    "## 5. Filter data\n",
    "\n",
    "The filter command creates a new RDD from another RDD based on a filter criteria.\n",
    "The filter syntax is: \n",
    "\n",
    "`.filter(lambda line: \"Filter Criteria Value\" in line)`\n",
    "\n",
    "Hint: Use a simple python `print` command to add a string to your Spark results and to run multiple actions in single cell.\n",
    "\n",
    "Find the number of instances of the word `Line` in the `z_str_rdd_split_flatmap` RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The count of words Line\n",
      "Is: 3\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "words_rd3 = z_str_rdd_split_flatmap.filter(lambda line: \"Line\" in line) \n",
    "\n",
    "print (\"The count of words \" + str(words_rd3.first()))\n",
    "print (\"Is: \" + str(words_rd3.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile\"></a>\n",
    "## 6. Analyze text data from a file\n",
    "In this section, you'll use a text file `README.txt` to create an RDD from it, and analyze the text in it. The file should already exist on `files`folder next to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile2\"></a>\n",
    "### 6.2 Create an RDD from the file\n",
    "Use the `textFile` method to create an RDD named `textfile_rdd` based on the `README.txt` file. The RDD will contain one element for each line in the `README.txt` file.\n",
    "Also, count the number of lines in the RDD, which is the same as the number of lines in the text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textfile_rdd = spark.sparkContext.textFile(\"files/README.txt\")\n",
    "textfile_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile3\"></a>\n",
    "### 6.3 Filter for a word \n",
    "Filter the RDD to keep only the elements that contain the word \"Spark\" with the `filter` transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "Spark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)\n",
    "Spark_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `'# Apache Spark'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of elements in this filtered RDD and present the result as a concatenated string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file README.txt has 19 of 98 Lines with word Spark in it.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "print (\"The file README.txt has \" + str(Spark_lines.count()) + \\\n",
    "\" of \" + str(textfile_rdd.count()) + \\\n",
    "\" Lines with word Spark in it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `The file README.txt has 19 of 98 Lines with word Spark in it.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile4\"></a>\n",
    "### 6.4 Count the instances of a string at the beginning of words\n",
    "Count the number of times the substring \"Spark\" appears at the beginning of a word in the original text.\n",
    "\n",
    "Here's what you need to do: \n",
    "\n",
    "1. Run a `flatMap` transformation on the Spark_lines RDD and split on white spaces.\n",
    "2. Create an RDD with key-value pairs where the first element of the tuple is the word and the second element is the number 1.\n",
    "3. Run a `reduceByKey` method with the `add` function to count the number of instances of each word.<br>\n",
    "4. Filter the resulting RDD to keep only the elements that start with the word \"Spark\". In Python, the syntax to determine whether a string starts with a token is: `string.startswith(\"token\")` \n",
    "5. Display the resulting list of elements that start with \"Spark\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 14),\n",
       " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " ('SparkPi', 2),\n",
       " ('Spark](#building-spark).', 1),\n",
       " ('Spark.', 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: write your code here.\n",
    "Spark_lines_words = Spark_lines.flatMap(lambda line: line.split(\" \"))\n",
    "Spark_line_word_pairs = Spark_lines_words.map(lambda word: (word, 1))\n",
    "Spark_line_word_reduced = Spark_line_word_pairs.reduceByKey(add)\n",
    "Spark_SparkWords = Spark_line_word_reduced.filter(lambda words: words[0].startswith(\"Spark\"))\n",
    "Spark_SparkWords.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:<br>\n",
    "<pre>\n",
    "[('Spark', 14),\n",
    " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
    " ('SparkPi', 2),\n",
    " ('Spark](#building-spark).', 1),\n",
    " ('Spark.', 1)]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wordfile5\"></a>\n",
    "### 6.5 Count instances of a string within words\n",
    "Now filter and display the elements that contain the substring \"Spark\" anywhere in the word, instead of just at the beginning of words like the last section. Your result should be a superset of the previous result.\n",
    "\n",
    "The Python syntax to determine whether a string contains a particular token is: `\"token\" in string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 14),\n",
       " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
       " ('SparkPi', 2),\n",
       " ('Spark](#building-spark).', 1),\n",
       " ('Spark.', 1),\n",
       " ('tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).',\n",
       "  1)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: write your code here.\n",
    "Spark_SparkInString = Spark_line_word_reduced.filter(lambda lines: \"Spark\" in lines[0])\n",
    "Spark_SparkInString.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:\n",
    "<pre>\n",
    "[('Spark', 14),\n",
    " ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n",
    " ('SparkPi', 2),\n",
    " ('Spark](#building-spark).', 1),\n",
    " ('Spark.', 1),\n",
    " ('tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).',\n",
    "  1)]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"numfile\"></a>\n",
    "## 7. Analyze numeric data from a file\n",
    "You'll analyze a sample file `Scores.txt` given in `files` folder that contains instructor names and scores. The file has the following format: Instructor Name,Score1,Score2,Score3,Score4,... The number of scores for each instructor could be diferent.\n",
    "Here is an example line from the text file: \"Carlo,5.5,3,3,4\" or \"Pablo,9,10,8.6,7,9,5,6\"\n",
    "Your task is to look at all the scores from each instructor and find the maximum score given by each instructor:\n",
    "\n",
    "1. Load the text file into an RDD.\n",
    "1. Run a transformation to create an RDD with the instructor names and the scores per instructor.\n",
    "1. Run a second transformation to compute the maximum score for each instructor.\n",
    "1. Display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tobias', 8.0),\n",
       " ('Malin', 10.0),\n",
       " ('Ali', 8.7),\n",
       " ('Magnus', 5.0),\n",
       " ('Alice', 9.1),\n",
       " ('Jack', 7.4)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: write your code here.\n",
    "scores = spark.sparkContext.textFile(\"./files/Scores.txt\")\n",
    "#scores_split = scores.map(lambda line: line.split(\",\"))\n",
    "scores_split = scores.map(lambda line: line.split(\",\")).map(lambda item: (item[0], list(map(float, item[1:]))))\n",
    "scores_max = scores_split.map(lambda item: (item[0], max(item[1])))\n",
    "scores_max.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see:\n",
    "<pre>\n",
    "[('Tobias', 8.0),\n",
    " ('Malin', 10.0),\n",
    " ('Ali', 8.7),\n",
    " ('Magnus', 5.0),\n",
    " ('Alice', 9.1),\n",
    " ('Jack', 7.4)]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. PageRank Algorithm\n",
    "\n",
    "In the final task, we are interested in using Spark to rank a list of websites based on their importance. One obvious application of such an analysis is to provide the ordering for web searches. To measure the importance of a page, you are tasked to calculate `PageRank`. PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites. The PageRank algorithm is very well described in Chapter 5 of the book `Mining Massive Data Sets`. You can access the book through [this link](http://www.mmds.org/).\n",
    "\n",
    "The websites are stored in a file named `urls.txt` located in the `files` folder. The input file has the following format:\n",
    "\n",
    "<pre>\n",
    "URL, neighbor URL\n",
    "URL, neighbor URL\n",
    "URL, neighbor URL\n",
    "...\n",
    "</pre>\n",
    "\n",
    "\n",
    "In the output, sort the websites descending based on their level of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) wikipedia.org 1.8801542122561585\n",
      "2) facebook.com 1.8695112819484958\n",
      "3) google.com 1.8569424318579348\n",
      "4) youtube.com 1.7366023592492508\n",
      "5) twitter.com 1.551959269674124\n",
      "6) imdb.com 1.4526237683775887\n",
      "7) amazon.com 1.4387122848873743\n",
      "8) merriam-webster.com 1.1918822764943973\n",
      "9) fandom.com 1.0830606534774072\n",
      "10) tripadvisor.com 1.0561827281336327\n",
      "11) apple.com 0.8859253344873649\n",
      "12) urbandictionary.com 0.8422447751832345\n",
      "13) wiktionary.org 0.6570191482586496\n",
      "14) instagram.com 0.41425018088168963\n",
      "15) blocket.se 0.3649535937812872\n",
      "16) hh.se 0.3174010337645914\n",
      "17) yelp.com 0.26761767060369474\n",
      "18) visithalland.com 0.09843614827632237\n",
      "19) halmstad.se 0.03452084840679059\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import BlockMatrix\n",
    "from pyspark.mllib.linalg import Matrices\n",
    "import numpy as np\n",
    "from operator import add\n",
    "\n",
    "def calcM(page, unique_urls):\n",
    "    ret = [0]*len(unique_urls)\n",
    "    for i, url in enumerate(unique_urls):\n",
    "        if url in page[1]:\n",
    "            ret[i] = 1/len(page[1])\n",
    "    return ret\n",
    "\n",
    "# Load Data\n",
    "pages = spark.sparkContext.textFile(\"./files/urls_link.txt\")\n",
    "# Split each row and filter out the entries where URL == Neighbor\n",
    "pages_split = pages.map(lambda page: page.split(\", \")).filter(lambda url: url[0] != url[1])\n",
    "# Pair and sort\n",
    "pages_paired = pages_split.map(lambda line: (line[0], line[1].split())).sortBy(lambda line: line)\n",
    "# Reduce everything so one URL has a list of neighbours, all sorted\n",
    "pages_reduced = pages_paired.reduceByKey(add).map(lambda item: [item[0], sorted(list(set(item[1])))])\n",
    "# Get the URLs\n",
    "unique_urls = pages_reduced.map(lambda url: url[0]).collect()\n",
    "\n",
    "# Number of unique URLs\n",
    "n = pages_reduced.count()\n",
    "# Create V0 matrix with every number initiated to 1, according to the book you can also init it to 1/n\n",
    "v = Matrices.dense(n, 1, [1]*n)\n",
    "# Calculate all the data necessary for M\n",
    "M_data = pages_reduced.flatMap(lambda page: calcM(page, unique_urls)).collect()\n",
    "# Create the M-Matrix\n",
    "M = Matrices.dense(n,n, M_data)\n",
    "\n",
    "# To do matrix multiplication you need to create a BlockMatrix\n",
    "# Following code creates a (n,1) for V and a (n,n) for M\n",
    "v_block = sc.parallelize([((0,0), v)])\n",
    "m_block = sc.parallelize([((0,0), M)])\n",
    "v = BlockMatrix(v_block, n, 1)\n",
    "M = BlockMatrix(m_block, n, n)\n",
    "\n",
    "# Do the calclations\n",
    "for i in range(20):\n",
    "    v = M.multiply(v)\n",
    "\n",
    "# Format the score to print the results\n",
    "scores = v.toLocalMatrix().toArray().reshape(n)\n",
    "\n",
    "for num, i in enumerate(np.argsort(-scores)):\n",
    "    print(\"{})\".format(num+1), unique_urls[i], scores[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PageRank` algorithm has some variants. Based on how you implement the algorithm and what parameters you choose, you will end up geting different scores and ranks. Following is one example output:\n",
    "\n",
    "<pre>\n",
    "1) google.com PageRank:1.8086487117025336\n",
    "2) wikipedia.org PageRank:1.8086487117025336\n",
    "3) facebook.com PageRank:1.777648228912891\n",
    "4) youtube.com PageRank:1.7021245199883723\n",
    "5) twitter.com PageRank:1.5510244219266531\n",
    "6) amazon.com PageRank:1.3904212544003804\n",
    "7) imdb.com PageRank:1.3008511398069642\n",
    "8) merriam-webster.com PageRank:1.1146111418335307\n",
    "9) fandom.com PageRank:1.002580594391505\n",
    "10) tripadvisor.com PageRank:0.9650177901770668\n",
    "11) apple.com PageRank:0.889833856208652\n",
    "12) urbandictionary.com PageRank:0.783325788483356\n",
    "13) wiktionary.org PageRank:0.7344188304535614\n",
    "14) instagram.com PageRank:0.488708566583417\n",
    "15) blocket.se PageRank:0.4475515972949302\n",
    "16) yelp.com PageRank:0.40433532158209406\n",
    "17) hh.se PageRank:0.3986772304305001\n",
    "18) visithalland.com PageRank:0.2340398586683635\n",
    "19) halmstad.se PageRank:0.19753243545269022\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "__IMPORTANT__ \n",
    "\n",
    "Please complete this Jupyter Notebook file and send it to mahmoud.rahat@hh.se within __two weeks__ after this lab session. Make sure that you include \"__Big Data Parallel Programming 2020__\" in your email title and write the name of the lab in the title of your email, e.g., Lab1 (do not put space between Lab and its number). __Change the name of your notebook file__ to include the `course title`, `lab title`, `first and last names` of you and your group mates, if any, e.g., \"`BDPP_Lab1_NameSurname_NameSurname.ipynb`\".\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
